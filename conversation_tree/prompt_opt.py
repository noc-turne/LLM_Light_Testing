from enum import Enum
import os
import logging
from openai import OpenAI
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

from ..utils import *


logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", 
    datefmt="%Y-%m-%d %H:%M:%S"
)
current_file = os.path.splitext(os.path.basename(__file__))[0]
logger = logging.getLogger(current_file)

class UserPromptGenerator(Enum):
    preset = 1
    AI = 2
    user = 3

def generate_sys_prompt(identity, topic):
    if(identity == 'user'):
        sys_prompt = f"ä½ æ˜¯ç”¨æˆ·ï¼Œä½ çš„èº«ä»½æ˜¯ä¸€åå¥³å¤§å­¦ç”Ÿï¼Œè¯·ä½ é’ˆå¯¹è¯é¢˜'{topic}'è¿›è¡Œå¯¹è¯ï¼Œä½ çš„æ€§æ ¼æ´»æ³¼å¼€æœ—ã€‚"
    elif(identity == 'AI'):
        sys_prompt = f"ä½ æ˜¯AIï¼Œé’ˆå¯¹è¯é¢˜'{topic}'è¿›è¡Œå¯¹è¯ã€‚"
    else:
        logger.error("Wrong Identity")
        sys_prompt = ""
    return {"role": "system", "content": sys_prompt}

def choose_from_preset(topic):
    data = {
        "é—®è¯¢æé—®": "è¯·ä½ å¸®æˆ‘è§„åˆ’ä¸€ä¸‹å»éŸ©å›½çš„æ—…æ¸¸è®¡åˆ’ã€‚",
        "æ•·è¡": "å“ˆå“ˆå“ˆ",
        "å¯»æ±‚å®‰æ…°": "æˆ‘å¤±æ‹äº†ï¼Œæ„Ÿè§‰ç”Ÿæ´»å¤±å»äº†è‰²å½©ï¼Œå“ã€‚",
        "æ‹’ç»å›å¤": "åˆ«é—®äº†ã€‚",
        "è¡¨æƒ…åŒ…å›å¤": "ğŸ˜“"
    }
    return data.get(topic, "é»˜è®¤å›å¤")

def call_ai(messages, sys_prompt, model_url="http://14.103.16.79:11000/v1", model_name="llama-3.3-70B-instruct"):
    try:
        client = OpenAI(api_key="token-123", base_url=model_url)

        msg = messages[:]
        msg.insert(0, sys_prompt)

        response = client.chat.completions.create(
            model=model_name,
            messages=msg,
            temperature=0.7,
            top_p=0.8,
            max_tokens=512,
            extra_body={
                "repetition_penalty": 1.05,
            },
        )

        answer = response.choices[0].message.content
        return answer
    except Exception as e:
        logger.error(f"AI call failed: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†è¿™ä¸ªè¯·æ±‚ã€‚"

def extend_tree(background_name, messages, topic_hist_list, extend_num, save_path):
    msg = messages[:]
    for _ in range(extend_num):
        user_prompt_text = call_ai(msg, generate_sys_prompt('user', topic_hist_list[-1]))
        user_prompt = {"role": "user", "content": user_prompt_text}
        msg.append(user_prompt)

        response_text = call_ai(msg, generate_sys_prompt("AI", topic_hist_list[-1]), model_url="http://14.103.16.79:11001/v1", model_name="Qwen25_72B_instruct")
        response = {"role": "assistant", "content": response_text}
        msg.append(response)

    file_name = background_name
    for topic in topic_hist_list:
        file_name += f"_{topic}"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name += f"_{extend_num}_{timestamp}.txt"

    os.makedirs(save_path, exist_ok=True)
    file_path = os.path.join(save_path, file_name)
    with open(file_path, 'w', encoding="utf-8") as f:
        json.dump(msg, f, indent=4, ensure_ascii=False)
    logger.info(f"done {file_name}")

def process_topic(background_name, messages, topic, topic_chosen_list, save_path, expand_num, extend_num, user_prompt_generator, preset_user_prompt_dict):
    local_messages = messages[:]
    topic_hist_list = [topic]

    # Generate user prompt
    if user_prompt_generator == UserPromptGenerator.preset:
        assert preset_user_prompt_dict is not None, "Error: preset_user_prompt_dict is None"
        user_prompt_text = preset_user_prompt_dict[topic]
    elif user_prompt_generator == UserPromptGenerator.AI:
        user_prompt_text = call_ai(local_messages, generate_sys_prompt('user', topic))
    else:
        user_prompt_text = input(f"è¯·è¾“å…¥é’ˆå¯¹è¯é¢˜'{topic}'çš„å†…å®¹ï¼š")

    user_prompt = {"role": "user", "content": user_prompt_text}
    local_messages.append(user_prompt)

    # Get AI response
    response_text = call_ai(local_messages, generate_sys_prompt("AI", topic), model_url="http://14.103.16.79:11001/v1", model_name="Qwen25_72B_instruct")
    response = {"role": "assistant", "content": response_text}
    local_messages.append(response)

    # Continue DFS
    dfs_generate_tree(
        background_name=background_name,
        messages=local_messages,
        topic_hist_list=topic_hist_list,
        depth=1,
        topic_chosen_list=topic_chosen_list,
        save_path=save_path,
        expand_num=expand_num,
        extend_num=extend_num,
        user_prompt_generator=user_prompt_generator
    )

def dfs_generate_tree(background_name, messages, topic_hist_list, depth, topic_chosen_list, save_path, expand_num=2, extend_num=6, user_prompt_generator=UserPromptGenerator.AI, preset_user_prompt_dict=None):
    """Recursive DFS tree generation with parallel processing at the first layer.

    Args:
        background_name (str): åœºæ™¯åç§°
        messages (list): å†å²å¯¹è¯
        topic_hist_list (list): å½“å‰æ¢ç´¢ä½ç½®ä¹‹å‰çš„topic_list
        depth (int): å½“å‰æ‰€åœ¨æ ‘çš„æ·±åº¦
        topic_chosen_list (list): æ ‘æ‹“å±•æ—¶å¯é€‰çš„topic
        save_path (str): ä¿å­˜è·¯å¾„
        expand_num (int, optional): æ ‘æ‹“å±•çš„è½®æ•°. Defaults to 2.
        extend_num (int, optional): æ ‘å»¶ä¼¸çš„è½®æ•°. Defaults to 6.
        user_prompt_generator (UserPromptGenerator, optional): user_promptç”±è°æ¥äº§ç”Ÿ. Defaults to UserPromptGenerator.preset.
    """
    if depth == expand_num:
        extend_tree(background_name, messages, topic_hist_list, extend_num, save_path)
        return

    if depth == 0:
        # Parallelize the first layer
        with ThreadPoolExecutor(max_workers=min(10, len(topic_chosen_list))) as executor:
            futures = [
                executor.submit(
                    process_topic, 
                    background_name, 
                    messages, 
                    topic, 
                    topic_chosen_list, 
                    save_path, 
                    expand_num, 
                    extend_num, 
                    user_prompt_generator
                )
                for topic in topic_chosen_list
            ]
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Error processing topic: {e}")
    else:
        for topic in topic_chosen_list:
            topic_hist_list.append(topic)
            if user_prompt_generator == UserPromptGenerator.preset:
                assert preset_user_prompt_dict is not None, "Error: preset_user_prompt_dict is None"
                user_prompt_text = preset_user_prompt_dict[topic]
            elif user_prompt_generator == UserPromptGenerator.AI:
                user_prompt_text = call_ai(messages, generate_sys_prompt('user', topic))
            else:
                user_prompt_text = input(f"è¯·è¾“å…¥é’ˆå¯¹è¯é¢˜'{topic}'çš„å†…å®¹ï¼š")

            user_prompt = {"role": "user", "content": user_prompt_text}
            messages.append(user_prompt)

            response_text = call_ai(messages, generate_sys_prompt("AI", topic), model_url="http://14.103.16.79:11001/v1", model_name="Qwen25_72B_instruct")
            response = {"role": "assistant", "content": response_text}
            messages.append(response)

            dfs_generate_tree(
                background_name, 
                messages, 
                topic_hist_list, 
                depth + 1, 
                topic_chosen_list, 
                save_path, 
                expand_num, 
                extend_num, 
                user_prompt_generator
            )

            messages.pop()  # ç§»é™¤ AI çš„å“åº”
            messages.pop()  # ç§»é™¤ç”¨æˆ·çš„æç¤º
            topic_hist_list.pop()

if __name__ == "__main__":

    config_file = "conversation_config.json"
    config = load_json_file(config_file)

    background_name = config.get("background_name", "")

    background_conversation_file = config.get("background_conversation_file", "")
    background_prompt = load_json_txt_prompt(background_conversation_file)

    topic_chosen_file = config.get("topic_chosen_file", "")
    topic_chosen_list = load_json_file(topic_chosen_file)
    assert isinstance(topic_chosen_list, list), "Error: 'topic_chosen_list' must be a list."

    save_path = config.get("save_path", "")

    user_prompt_generator = config.get("user_prompt_generator_type", "AI")
    preset_user_prompt_dict = None
    if user_prompt_generator.lower() == "user":
        user_prompt_generator_type = UserPromptGenerator.user
    elif user_prompt_generator.lower() == "preset":
        user_prompt_generator_type = UserPromptGenerator.preset

        preset_user_prompt_file = config.get("preset_user_prompt_file", "")
        preset_user_prompt_dict = load_json_file(preset_user_prompt_file)
        assert isinstance(preset_user_prompt_dict, dict), "Error: 'preset_user_prompt_dict' must be a dict"

        assert topic_chosen_list == preset_user_prompt_dict.keys(), "Error: topic_chosen_list != preset_user_prompt_dict.keys()"
    else:
        user_prompt_generator_type = UserPromptGenerator.AI

    dfs_generate_tree(
        background_name=background_name, 
        messages=background_prompt, 
        topic_hist_list=[], 
        depth=0, 
        topic_chosen_list=topic_chosen_list, 
        save_path=save_path, 
        expand_num=2, 
        extend_num=6, 
        user_prompt_generator=user_prompt_generator_type,
        preset_user_prompt_dict=preset_user_prompt_dict
    )
